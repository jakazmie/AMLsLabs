{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 1 - Experimenting in a local environment\n",
    "In this lab you will experiment with a `scikit-learn` machine learning classification algorithms using a small development dataset and local storage and compute. In many cases, your local development environment will not have enough computational and storage resources to support training on full datasets. A common machine learning workflow pattern is to develop and debug your training scripts in a local environment and then run training jobs on full datasets using powerfull cloud compute resources.\n",
    "\n",
    "You will use Azure Machine Learning service Experiment to track your training runs. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to AML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AML Experiment\n",
    "\n",
    "In Azure Machine Learning service, you can track metrics and other artifacts created during model development process. The tracked items are stored in *Experiments* and organized in *Runs*. A *run* is a single trial of an experiment. A *run* object is used to store output of the trial, and to analyze results and access artifacts generated by the trial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "\n",
    "# Create AML Experiment\n",
    "experiment_name = 'aerial-train-in-notebook'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment with `scikit-learn` classification models\n",
    "\n",
    "You will experiment with `scikit-learn` models using raw image data as input features. This is a little bit of a naive approach as experience teaches us that simple machine learning models don't perform well on raw image data unless dealing with really simplistic scenarios like the MNIST dataset. Nevertheleess, we will use this approach to demostrate how to track the training progress using AML Experiment and Run objects. In the following labs you will utilize Transfer Learning to train much better classifiers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dowload the development dataset\n",
    "\n",
    "The datasets used in the labs have been uploaded to a public container in Azure Blob Storage.\n",
    "\n",
    "Download the small development dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "\n",
    "wget -nv https://azureailabs.blob.core.windows.net/aerialtar/aerial-tiny.zip -P /tmp\n",
    "unzip -q /tmp/aerial-tiny.zip -d /tmp\n",
    "ls -l /tmp/aerial-tiny"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is organized into six folders, each folder containing images of a given land class."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load and label images\n",
    "\n",
    "Load the images to a `numpy` array and assign numeric labels representing land classes. \n",
    "."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# Define a utility function to load images from a folder\n",
    "def load_images(input_dir):\n",
    "    label_to_integer = {\n",
    "        \"Barren\": 0,\n",
    "        \"Cultivated\": 1,\n",
    "        \"Developed\": 2,\n",
    "        \"Forest\": 3,\n",
    "        \"Herbaceous\": 4,\n",
    "        \"Shrub\": 5}\n",
    "    \n",
    "    image_list = [(np.array(Image.open(os.path.join(input_dir, folder, filename))), label_to_integer[folder])\n",
    "             for folder in os.listdir(input_dir)\n",
    "             for filename in os.listdir(os.path.join(input_dir, folder))]\n",
    "    \n",
    "    images, labels = zip(*image_list)\n",
    "    \n",
    "    return np.asarray(images), np.asarray(labels)\n",
    "\n",
    "\n",
    "# Load images\n",
    "images_dir = '/tmp/aerial-tiny'\n",
    "images, labels = load_images(images_dir)\n",
    "\n",
    "print(images.shape)\n",
    "print(labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The images are in the `PNG` format. After loading, the images are represented by rank 3 tensors of shape `(224, 224, 3)`. The color encoding is `RGB`.\n",
    "\n",
    "In this notebook you will experiment with `sklearn` classification models. Most `sklearn` algorithms require input feature to be represented by rank 1 tensors - or vectors. Since our images are rank 3 tensors `(224, 224, 3)` we need to flatten them to `(150528,)` shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape the images\n",
    "X = np.ndarray.reshape(images, (images.shape[0], -1))\n",
    "\n",
    "print(\"Input data:\")\n",
    "print(\"  Images: \", X.shape)\n",
    "print(\"  Labels: \", labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Logistic Regression model\n",
    "\n",
    "We will start with a simple logistic regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Start training\n",
    "The below code snippet uses AML Experiment and Run objects to log values of hyperparametrs, evaluation metrics, and the serialized model. Note that the training process runs within the notebook's kernel but the tracked artifacts are pushed to the cloud. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn import metrics\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Divide the dataset in training and validation\n",
    "X_train, X_validate, y_train, y_validate = train_test_split(X, labels,\n",
    "                                                           test_size=0.2,\n",
    "                                                           shuffle=True,\n",
    "                                                           random_state=1,\n",
    "                                                           stratify=labels)\n",
    "\n",
    "# Initialize Experiment logging\n",
    "run = exp.start_logging()\n",
    "\n",
    "# Log run description and hyper-parameter values\n",
    "run.tag(\"Description\", \"Naive attempt to fit logistic regression to aerial image data\")\n",
    "run.log(\"Algorithm\", \"logistic regresion\")\n",
    "run.log(\"Hyperparameter:Solver\", \"lbfgs\")\n",
    "run.log(\"Hyperparameter:C\", 1.0)\n",
    "\n",
    "# Train logistic regression\n",
    "print(\"Starting training ...\")\n",
    "lr = LogisticRegression(\n",
    "    multi_class='multinomial',\n",
    "    solver='lbfgs',\n",
    "    C = 1.0,\n",
    "    verbose=1)\n",
    "\n",
    "lr.fit(X_train, y_train)\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Evaluate the model on validation images\n",
    "print(\"Starting evaluation\")\n",
    "y_hat = lr.predict(X_validate)\n",
    "val_accuracy = np.average(y_hat == y_validate)\n",
    "print(\"Validation accuracy:\", val_accuracy)\n",
    "run.log('Validation accuracy', val_accuracy)\n",
    "\n",
    "# Save and upload the model\n",
    "joblib.dump(value=lr, filename='model.pkl')\n",
    "run.upload_file(name='outputs/model.pkl', path_or_stream='./model.pkl')\n",
    "\n",
    "# Finalize the run\n",
    "run.complete()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can browse the recorded run in Azure portal. The run's hyperparameters, performance measures, and the serialized model are all stored in the Experiment.\n",
    "\n",
    "As shown by the validation accuracy, our model's performance is rather absymal. Logistic regression can only learn linear decision boundries and cannot handle a complex dataset like our land images. Let's try an ML algorithm with more capacity - Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train Random Forest model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Initialize logging\n",
    "run = exp.start_logging()\n",
    "\n",
    "# Log run description and hyper-parameter values\n",
    "run.tag(\"Description\", \"Another naive attempt to train on aerial image data - random forests\")\n",
    "run.log(\"No of trees\", 100)\n",
    "run.log(\"Max Depth\", 7)\n",
    "\n",
    "# Train logistic regression\n",
    "print(\"Starting training ...\")\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=7,\n",
    "    verbose=1)\n",
    "\n",
    "rf.fit(X_train, y_train)\n",
    "print(\"Training completed.\")\n",
    "\n",
    "# Evaluate the model on validation images\n",
    "print(\"Starting evaluation\")\n",
    "y_hat = rf.predict(X_validate)\n",
    "val_accuracy = np.average(y_hat == y_validate)\n",
    "print(\"Validation accuracy:\", val_accuracy)\n",
    "run.log('Validation accuracy', val_accuracy)\n",
    "\n",
    "# Save and upload the model\n",
    "joblib.dump(value=lr, filename='model.pkl')\n",
    "run.upload_file(name='outputs/model.pkl', path_or_stream='./model.pkl')\n",
    "\n",
    "# Finalize the run\n",
    "run.complete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is much better than logistic regression but still pretty bad. We could attempt to fine-tune hyper-parameters or try other machine learning algorithms but rather thank pursuing this naive approach we will apply a proven technique that has emerged in the recent years - Transfer Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "\n",
    "In the next lab you will utilize a pre-trained deep neural network to extract powerful features from images and use them to train a better performing classifier.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
