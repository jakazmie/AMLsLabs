{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2 - Feature Engineering\n",
    "In this lab we will develop and run a Python script that pre-processes our image set into a set of powerful features - sometimes referred to as bottleneck features.\n",
    "\n",
    "To create bottleneck features we will utilize a pre-trained Deep Learning network that was trained on a general computer vision domain. \n",
    "\n",
    "As explained by your instructor this approach is called Transfer Learning. Transfer Learning is a powerful Machine Learning technique that is based on an observation that the knowledge gained while solving one problem can be applied to a different (but related problem).\n",
    "\n",
    "In the context of an image classification task, a DNN trained on one visual domain can accelerate learing in another visual domain. Although, our pre-trained network does not know how to classify aerial land plot images, it knows enough about representing image concepts that if we use it to pre-process aerial images, the extracted image features can be used to effectively train a relatively simple classifier on a **limited number** of samples.\n",
    "\n",
    "The below diagram represents the architecture of our solution.\n",
    "\n",
    "![Transfer Learning](https://github.com/jakazmie/images-for-hands-on-labs/raw/master/tlcl.png)\n",
    "\n",
    "We will use **ResNet50** trained on **imagenet** dataset to extract features. We will occasionally refer to this component of the solution as a featurizer. The output of the featurizer is a vector of 2048 floating point numbers, each representing a feature extracted from an image. \n",
    "\n",
    "We will then use extracted features to train an scikit-learn classifier. (next lab).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDK version: 1.0.2\n"
     ]
    }
   ],
   "source": [
    "# Check core SDK version number\n",
    "import azureml.core\n",
    "print(\"SDK version:\", azureml.core.VERSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Connect to AML Workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found the config file in: /Users/jarekk/repos/jakazmie/AMLsLabs/01-aml-walkthrough/aml_config/config.json\n",
      "jkamlslab\n",
      "jkamlslab\n",
      "eastus2\n",
      "952a710c-8d9c-40c1-9fec-f752138cc0b3\n"
     ]
    }
   ],
   "source": [
    "import azureml.core\n",
    "from azureml.core import Workspace\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "print(ws.name, ws.resource_group, ws.location, ws.subscription_id, sep='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create AML Experiment\n",
    "We will track runs of the feature engineering script in a dedicated Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Experiment\n",
    "experiment_name = 'aerial-feature-engineering'\n",
    "exp = Experiment(workspace=ws, name=experiment_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data pre-processing script\n",
    "\n",
    "The Python script processes an input image dataset into an output bottleneck feature set. The script expects the images to be organized in the below folder structure:\n",
    "```\n",
    "Barren/\n",
    "Cultivated/\n",
    "Developed/\n",
    "Forest/\n",
    "Herbaceous/\n",
    "Shrub/\n",
    "```\n",
    "\n",
    "The location of the input dataset and the location where to save the output dataset are passed to the script as command line parameters. The output dataset will be stored in a binary HDF5 data format used commonly in Machine Learning and High Performance Computing solutions.\n",
    "\n",
    "The script is designed to work with a large number of images. As such it does not load all input images to memory at once. Instead it utilizes a utility function `load_images` to feed the featurizer. The function yields batches of images - as Numpy arrays - preprocessed to the format required by **ResNet50**. \n",
    "\n",
    "We will not attempt to run the script on a full dataset in a local environment. It is very computationally intensive and unless you run it in an evironment equipped with a powerful GPU it would be very slow. \n",
    "\n",
    "However, we will demonstrate how to run the script locally using the same small development dataset we used in the previous lab. Running the script locally under the control of Azure ML can be very usefull during development and debugging.\n",
    "\n",
    "To process the full dataset we will execute the script on a remote Azure ML Compute equipped with NVidia GPU."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a folder to hold the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "script_folder = './script'\n",
    "os.makedirs(script_folder, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use Jupyter `%%writefile` magic to write the script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ./script/extract.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile $script_folder/extract.py\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "import h5py\n",
    "from tqdm import tqdm\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import azureml.contrib.brainwave.models.utils as utils\n",
    "from azureml.contrib.brainwave.models import QuantizedResnet50\n",
    "\n",
    "\n",
    "def get_batch(pathnames, batchsize=64):\n",
    "    \"\"\"Yield succesive batches of images\"\"\"\n",
    "    for i in range(0, len(pathnames), batchsize):\n",
    "        yield pathnames[i:i+batchsize]\n",
    "        \n",
    "\n",
    "def load_images(batch):\n",
    "    \"\"\"Return a batch of images as a list of bytes sequences\"\"\"\n",
    "    images = []\n",
    "    for path in batch:\n",
    "        with open(path, 'rb') as f:\n",
    "            images.append(f.read())\n",
    "    return images\n",
    "\n",
    "def create_bottleneck_features():\n",
    "    \"\"\"Createl bottleneck features and save them to H5 formatted file\"\"\"\n",
    "    img_dir = FLAGS.input_data_dir\n",
    "    \n",
    "    # Label images \n",
    "    \n",
    "    # Create the dictionary that maps class names into numeric labels   \n",
    "    label_map = {\n",
    "        \"Barren\": 0,\n",
    "        \"Cultivated\": 1,\n",
    "        \"Developed\": 2,\n",
    "        \"Forest\": 3,\n",
    "        \"Herbaceous\": 4,\n",
    "        \"Shrub\": 5}    \n",
    "\n",
    "    # Create a list of all images in a root folder with associated numeric labels\n",
    "    folders = list(label_map.keys())\n",
    "    labeled_image_list = [(os.path.join(img_dir, folder, image), label_map[folder]) \n",
    "                          for folder in folders \n",
    "                          for image in os.listdir(os.path.join(img_dir, folder))\n",
    "                              ]\n",
    "    # Shuffle the list\n",
    "    random.shuffle(labeled_image_list)\n",
    "    image_paths, labels = zip(*labeled_image_list)\n",
    "    \n",
    "    # Build featurizer graph\n",
    "    \n",
    "    # Convert input images (loaded as bytes sequences) into (224, 224, 3) tensors\n",
    "    # with pixel values in Caffe encoding\n",
    "    in_images = tf.placeholder(tf.string)\n",
    "    image_tensors = utils.preprocess_array(in_images)\n",
    "\n",
    "    # Create ResNet152 \n",
    "    model_path = os.path.expanduser('~/models')\n",
    "    resnet = QuantizedResnet50(model_path, is_frozen=True)\n",
    "\n",
    "    # Import ResNet152 graph\n",
    "    features = resnet.import_graph_def(input_tensor=image_tensors)\n",
    "    \n",
    "    # Generate bottleneck features\n",
    "    print(\"Generating bottleneck features\")\n",
    "    bottleneck_features = []\n",
    "    with tf.Session() as sess:\n",
    "        for paths in tqdm(get_batch(image_paths)):\n",
    "            image_batch = load_images(paths)\n",
    "            result = sess.run([features], feed_dict={in_images: image_batch})\n",
    "            result = np.reshape(result[0], (len(result[0]), 2048))\n",
    "            bottleneck_features.extend(result)\n",
    "        \n",
    "    bottleneck_features = np.array(bottleneck_features)\n",
    "    print(bottleneck_features.shape)\n",
    "        \n",
    "    # Save the bottleneck features to HDF5 file\n",
    "    filename = FLAGS.file_name\n",
    "    output_file = os.path.join(FLAGS.output_data_dir, filename)\n",
    "    labels = np.asarray(labels)\n",
    "    print(\"Saving bottleneck features to {}\".format(output_file))\n",
    "    print(\"   Features: \", bottleneck_features.shape)\n",
    "    print(\"   Labels: \", labels.shape)\n",
    "    with h5py.File(output_file, \"w\") as hfile:\n",
    "        features_dset = hfile.create_dataset('features', data=bottleneck_features)\n",
    "        labels_dset = hfile.create_dataset('labels', data=labels)\n",
    "    \n",
    "    print(\"Done\")\n",
    "\n",
    "FLAGS = tf.app.flags.FLAGS\n",
    "\n",
    "# Default global parameters\n",
    "tf.app.flags.DEFINE_integer('batch_size', 64, \"Number of images per batch\")\n",
    "tf.app.flags.DEFINE_string('input_data_dir', 'aerialtiny', \"Folder with training and validation images\")\n",
    "tf.app.flags.DEFINE_string('output_data_dir', 'bottleneck_features', \"A folder for saving bottleneck features\")\n",
    "tf.app.flags.DEFINE_string('file_name', 'aerial_bottleneck_resnet50.h5', \"Name of output training file\")\n",
    "\n",
    "\n",
    "def main(argv=None):\n",
    "    print(\"Starting\")\n",
    "    print(\"Reading images from:\", FLAGS.input_data_dir)\n",
    "    print(\"The output bottleneck file will be saved to:\", FLAGS.output_data_dir)\n",
    "\n",
    "    os.makedirs(FLAGS.output_data_dir, exist_ok=True)\n",
    "\n",
    "    create_bottleneck_features()\n",
    "  \n",
    "if __name__ == '__main__':\n",
    "    tf.app.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the script locally\n",
    "\n",
    "As noted in the introduction, we will first run the script locally"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Run environment\n",
    "We will use a user-managed run, which means we assume that all the necessary packages are already available in the Python environment selected to run the script. In our case this is true, as we pre-installed all the dependencies during the lab setup. Alternatively, you can execute a local run in system-managed environment. In that case AML would build a new conda environment and execute the script in it.\n",
    "\n",
    "*Make sure to modify the **interpreter_path** property to point to your Python environment. On DSVM this path is `/anaconda/envs/py36/bin/python`*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.runconfig import RunConfiguration\n",
    "\n",
    "run_config = RunConfiguration()\n",
    "run_config.environment.python.user_managed_dependencies = True\n",
    "#run_config.environment.python.interpreter_path = '/anaconda/envs/py36/bin/python'\n",
    "run_config.environment.python.interpreter_path = '/anaconda3/envs/aml/bin/python'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the script. \n",
    "Note that we need to supply an absolute path to the folder with training and validation images. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table style=\"width:100%\"><tr><th>Experiment</th><th>Id</th><th>Type</th><th>Status</th><th>Details Page</th><th>Docs Page</th></tr><tr><td>aerial-feature-engineering</td><td>aerial-feature-engineering_1544056301_dea019bf</td><td>azureml.scriptrun</td><td>Running</td><td><a href=\"https://mlworkspace.azure.ai/portal/subscriptions/952a710c-8d9c-40c1-9fec-f752138cc0b3/resourceGroups/jkamlslab/providers/Microsoft.MachineLearningServices/workspaces/jkamlslab/experiments/aerial-feature-engineering/runs/aerial-feature-engineering_1544056301_dea019bf\" target=\"_blank\" rel=\"noopener\">Link to Azure Portal</a></td><td><a href=\"https://docs.microsoft.com/en-us/python/api/azureml-core/azureml.core.script_run.ScriptRun?view=azure-ml-py\" target=\"_blank\" rel=\"noopener\">Link to Documentation</a></td></tr></table>"
      ],
      "text/plain": [
       "Run(Experiment: aerial-feature-engineering,\n",
       "Id: aerial-feature-engineering_1544056301_dea019bf,\n",
       "Type: azureml.scriptrun,\n",
       "Status: Running)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "src = ScriptRunConfig(\n",
    "    source_directory='./script',\n",
    "    script='extract.py',\n",
    "    run_config=run_config,\n",
    "    arguments=['--input_data_dir', '/tmp/aerial-tiny',\n",
    "               '--output_data_dir', '/tmp/bottleneck_features',\n",
    "               '--file_name', 'aerial_bottleneck_resnet50_brainwave.h5'])\n",
    "\n",
    "tags = {\"Compute target\": \"Local\", \"DNN\": \"ResNet50\"}\n",
    "run = exp.submit(src, tags=tags)\n",
    "run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block to wait till run finishes and stream the output. Check CPU utilization on your workstation. On Linux run `htop` utility in a Jupyter terminal window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunId: aerial-feature-engineering_1544056301_dea019bf\n",
      "\n",
      "Streaming azureml-logs/80_driver_log.txt\n",
      "========================================\n",
      "\n",
      "Starting\n",
      "Reading images from: /tmp/aerial-tiny\n",
      "The output bottleneck file will be saved to: /tmp/bottleneck_features\n",
      "2018-12-05 16:31:55.500224: I tensorflow/core/platform/cpu_feature_guard.cc:141] Your CPU supports instructions that this TensorFlow binary was not compiled to use: AVX2 FMA\n",
      "Converted 265 variables to const ops.\n",
      "Generating bottleneck features\n",
      "\n",
      "0it [00:00, ?it/s]\n",
      "1it [00:34, 34.69s/it]\n",
      "2it [00:56, 30.70s/it]\n",
      "3it [01:17, 27.83s/it]\n",
      "4it [01:38, 25.85s/it]\n",
      "5it [01:58, 24.24s/it]\n",
      "6it [02:19, 23.05s/it]\n",
      "7it [02:40, 22.40s/it]\n",
      "8it [03:00, 21.79s/it]\n",
      "9it [03:21, 21.53s/it]\n",
      "10it [03:42, 21.28s/it]\n",
      "11it [04:02, 21.06s/it]\n",
      "12it [04:23, 21.06s/it]\n",
      "13it [04:43, 20.77s/it]\n",
      "14it [05:04, 20.91s/it]\n",
      "15it [05:27, 21.53s/it]\n",
      "16it [05:49, 21.47s/it]\n",
      "17it [06:09, 20.97s/it]\n",
      "18it [06:30, 21.13s/it]\n",
      "19it [06:50, 20.69s/it]\n",
      "20it [07:10, 20.67s/it]\n",
      "21it [07:24, 18.65s/it]\n",
      "(1326, 2048)\n",
      "Saving bottleneck features to /tmp/bottleneck_features/aerial_bottleneck_resnet50_brainwave.h5\n",
      "   Features:  (1326, 2048)\n",
      "   Labels:  (1326,)\n",
      "Done\n",
      "\n",
      "\n",
      "The experiment completed successfully. Finalizing run...\n",
      "Logging experiment finalizing status in history service\n",
      "\n",
      "Execution Summary\n",
      "=================\n",
      "RunId: aerial-feature-engineering_1544056301_dea019bf\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'runId': 'aerial-feature-engineering_1544056301_dea019bf',\n",
       " 'target': 'local',\n",
       " 'status': 'Finalizing',\n",
       " 'startTimeUtc': '2018-12-06T00:31:42.227766Z',\n",
       " 'properties': {'azureml.runsource': 'experiment',\n",
       "  'ContentSnapshotId': 'f7c7a2f1-07ac-4572-ba3f-1afe825c3304'},\n",
       " 'runDefinition': {'Script': 'extract.py',\n",
       "  'Arguments': ['--input_data_dir',\n",
       "   '/tmp/aerial-tiny',\n",
       "   '--output_data_dir',\n",
       "   '/tmp/bottleneck_features',\n",
       "   '--file_name',\n",
       "   'aerial_bottleneck_resnet50_brainwave.h5'],\n",
       "  'SourceDirectoryDataStore': None,\n",
       "  'Framework': 0,\n",
       "  'Communicator': 0,\n",
       "  'Target': 'local',\n",
       "  'DataReferences': {},\n",
       "  'JobName': None,\n",
       "  'AutoPrepareEnvironment': True,\n",
       "  'MaxRunDurationSeconds': None,\n",
       "  'NodeCount': 1,\n",
       "  'Environment': {'Python': {'InterpreterPath': '/anaconda3/envs/aml/bin/python',\n",
       "    'UserManagedDependencies': True,\n",
       "    'CondaDependencies': {'name': 'project_environment',\n",
       "     'dependencies': ['python=3.6.2', {'pip': ['azureml-defaults']}]},\n",
       "    'CondaDependenciesFile': None},\n",
       "   'EnvironmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'Docker': {'BaseImage': 'mcr.microsoft.com/azureml/base:0.2.0',\n",
       "    'Enabled': False,\n",
       "    'SharedVolumes': True,\n",
       "    'Preparation': None,\n",
       "    'GpuSupport': False,\n",
       "    'Arguments': [],\n",
       "    'BaseImageRegistry': {'Address': None,\n",
       "     'Username': None,\n",
       "     'Password': None}},\n",
       "   'Spark': {'Repositories': ['https://mmlspark.azureedge.net/maven'],\n",
       "    'Packages': [{'Group': 'com.microsoft.ml.spark',\n",
       "      'Artifact': 'mmlspark_2.11',\n",
       "      'Version': '0.12'}],\n",
       "    'PrecachePackages': True}},\n",
       "  'History': {'OutputCollection': True},\n",
       "  'Spark': {'Configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'BatchAi': {'NodeCount': 0},\n",
       "  'AmlCompute': {'Name': None,\n",
       "   'VmSize': None,\n",
       "   'VmPriority': None,\n",
       "   'RetainCluster': False,\n",
       "   'ClusterMaxNodeCount': 1},\n",
       "  'Tensorflow': {'WorkerCount': 1, 'ParameterServerCount': 1},\n",
       "  'Mpi': {'ProcessCountPerNode': 1},\n",
       "  'Hdi': {'YarnDeployMode': 2},\n",
       "  'ContainerInstance': {'Region': None, 'CpuCores': 0, 'MemoryGb': 0},\n",
       "  'ExposedPorts': None,\n",
       "  'PrepareEnvironment': None},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': 'https://jkamlslastoragevfzvtchj.blob.core.windows.net/azureml/ExperimentRun/aerial-feature-engineering_1544056301_dea019bf/azureml-logs/60_control_log.txt?sv=2018-03-28&sr=b&sig=hKU1%2F7r1bhl5P85TU7KucmMZenZfPS3BooPrqkmgqO0%3D&st=2018-12-06T00%3A29%3A33Z&se=2018-12-06T08%3A39%3A33Z&sp=r',\n",
       "  'azureml-logs/80_driver_log.txt': 'https://jkamlslastoragevfzvtchj.blob.core.windows.net/azureml/ExperimentRun/aerial-feature-engineering_1544056301_dea019bf/azureml-logs/80_driver_log.txt?sv=2018-03-28&sr=b&sig=%2FjXYG%2FcNJCScV9VyfrV3uahS0qOfDJiimEMDajdUd5k%3D&st=2018-12-06T00%3A29%3A33Z&se=2018-12-06T08%3A39%3A33Z&sp=r',\n",
       "  'azureml-logs/azureml.log': 'https://jkamlslastoragevfzvtchj.blob.core.windows.net/azureml/ExperimentRun/aerial-feature-engineering_1544056301_dea019bf/azureml-logs/azureml.log?sv=2018-03-28&sr=b&sig=SGZDFbZZ92xNp7iC8r%2FhzQmpAOh3qXdZ3Eet11sSUQM%3D&st=2018-12-06T00%3A29%3A33Z&se=2018-12-06T08%3A39%3A33Z&sp=r'}}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logs from the run have been pushed to AML Experiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['azureml-logs/60_control_log.txt', 'azureml-logs/80_driver_log.txt', 'driver_log', 'azureml-logs/azureml.log']\n"
     ]
    }
   ],
   "source": [
    "print(run.get_file_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The bottleneck files can be found in a local directory passed to the run as a command line parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aerial_bottleneck_resnet50_brainwave.h5\n"
     ]
    }
   ],
   "source": [
    "%%sh\n",
    "\n",
    "ls /tmp/bottleneck_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the script on Azure Machine Learning Managed Compute\n",
    "\n",
    "As you can see, even on a really small dataset the processing is very slow. In the next step, you will run the script on a full dataset using Azure ML Managed Compute. \n",
    "\n",
    "Azure Machine Learning Managed Compute(AmlCompute) is a managed service that enables you to train machine learning models on clusters of Azure virtual machines, including VMs with GPU support. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Azure ML Managed Compute\n",
    "\n",
    "We will use a single *Standard_NC6* VM equipped with Tesla K80 GPU as Azure ML Compute target."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found compute target. just use it. gpucompute\n"
     ]
    }
   ],
   "source": [
    "from azureml.core.compute import AmlCompute\n",
    "from azureml.core.compute import ComputeTarget\n",
    "import os\n",
    "\n",
    "\n",
    "# choose a name for your cluster\n",
    "compute_name = os.environ.get(\"AML_COMPUTE_CLUSTER_NAME\", \"gpucompute\")\n",
    "compute_min_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MIN_NODES\", 1)\n",
    "compute_max_nodes = os.environ.get(\"AML_COMPUTE_CLUSTER_MAX_NODES\", 1)\n",
    "\n",
    "vm_size = os.environ.get(\"AML_COMPUTE_CLUSTER_SKU\", \"STANDARD_NC6\")\n",
    "\n",
    "\n",
    "if compute_name in ws.compute_targets:\n",
    "    compute_target = ws.compute_targets[compute_name]\n",
    "    if compute_target and type(compute_target) is AmlCompute:\n",
    "        print('found compute target. just use it. ' + compute_name)\n",
    "else:\n",
    "    print('creating a new compute target...')\n",
    "    provisioning_config = AmlCompute.provisioning_configuration(vm_size = vm_size,\n",
    "                                                                min_nodes = compute_min_nodes, \n",
    "                                                                max_nodes = compute_max_nodes)\n",
    "\n",
    "    # create the cluster\n",
    "    compute_target = ComputeTarget.create(ws, compute_name, provisioning_config)\n",
    "\n",
    "    # can poll for a minimum number of nodes and for a specific timeout. \n",
    "    # if no min node count is provided it will use the scale settings for the cluster\n",
    "    compute_target.wait_for_completion(show_output=True, min_node_count=None, timeout_in_minutes=20)\n",
    "\n",
    "     # For a more detailed view of current AmlCompute status, use the 'status' property    \n",
    "    print(compute_target.status.serialize())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configure Datastores \n",
    "The dataset we will use for training has been uploaded to a public Azure blob storage container. We will register this container as an AML Datastore within our workspace. Before the data prep script runs, the datastore's content - training images - will be copied to the local storage on the compute node.\n",
    "\n",
    "After the script completes, its output - the bottleneck features file - will be uploaded by AML to the workspace's default datastore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found existing datastore for input images: input_images\n",
      "input_images AzureBlob azureailabs aerial-med\n",
      "Using the default datastore for output: \n",
      "workspaceblobstore AzureBlob jkamlslastoragevfzvtchj azureml-blobstore-b9d096b6-8b2a-49bb-aef5-cc1bd0f6b751\n"
     ]
    }
   ],
   "source": [
    "from azureml.core import Datastore\n",
    "\n",
    "images_account = 'azureailabs'\n",
    "images_container = 'aerial-med'\n",
    "datastore_name = 'input_images'\n",
    "\n",
    "# Check if the datastore exists. If not create a new one\n",
    "try:\n",
    "    input_ds = Datastore.get(ws, datastore_name)\n",
    "    print('Found existing datastore for input images:', input_ds.name)\n",
    "except:\n",
    "    input_ds = Datastore.register_azure_blob_container(workspace=ws, datastore_name=datastore_name,\n",
    "                                            container_name=images_container,\n",
    "                                            account_name=images_account)\n",
    "    print('Creating new datastore for input images')\n",
    "\n",
    " \n",
    "   \n",
    "print(input_ds.name, input_ds.datastore_type, input_ds.account_name, input_ds.container_name)\n",
    "\n",
    "output_ds = ws.get_default_datastore()\n",
    "print(\"Using the default datastore for output: \")\n",
    "print(output_ds.name, output_ds.datastore_type, output_ds.account_name, output_ds.container_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start and monitor a remote run\n",
    "\n",
    "We will run a script in a docker that will be created automatically by AML and configured with the \n",
    "specified dependencies.\n",
    "\n",
    "The first run takes longer. The subsequent runs, as long as the script dependencies don't change, are much faster.\n",
    "\n",
    "You can check the progress of a running job in multiple ways: Azure Portal, AML Jupyter Widgets, log files streaming. We will use AML Jupyter Widgets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core.conda_dependencies import CondaDependencies\n",
    "from azureml.core.runconfig import DataReferenceConfiguration\n",
    "from azureml.core.runconfig import RunConfiguration\n",
    "from azureml.core import ScriptRunConfig\n",
    "\n",
    "script_folder = 'script'\n",
    "script_name = 'extract.py'\n",
    "output_dir = 'bottleneck_features'\n",
    "input_dir = 'aerial'\n",
    "\n",
    "# create a new RunConfig object\n",
    "run_config = RunConfiguration(framework=\"python\")\n",
    "\n",
    "# Set compute target to Azure ML compute and configure docker base image\n",
    "run_config.target = compute_target.name\n",
    "run_config.environment.docker.enabled = True\n",
    "run_config.environment.docker.base_image = azureml.core.runconfig.DEFAULT_GPU_IMAGE\n",
    "\n",
    "# specify CondaDependencies obj\n",
    "# pip_packages = ['azureml-sdk[contrib]']\n",
    "conda_packages = ['h5py', 'tqdm', 'tensorflow-gpu==1.10']\n",
    "#conda_dependencies = CondaDependencies.create(pip_packages=pip_packages, conda_packages=conda_packages)\n",
    "conda_dependencies = CondaDependencies.create(conda_packages=conda_packages)\n",
    "run_config.environment.python.conda_dependencies = conda_dependencies\n",
    "    \n",
    "\n",
    "# configure data references\n",
    "input_dr = DataReferenceConfiguration(datastore_name=input_ds.name, \n",
    "                   path_on_compute=input_dir,                   \n",
    "                   mode='download', # download files from datastore to compute target\n",
    "                   overwrite=True)\n",
    "\n",
    "output_dr = DataReferenceConfiguration(datastore_name=output_ds.name, \n",
    "                   path_on_datastore=output_dir, \n",
    "                   path_on_compute=output_dir,\n",
    "                   mode='upload', # upload files from the compute to datastore\n",
    "                   overwrite=True)\n",
    "\n",
    "run_config.data_references = {input_ds.name: input_dr, output_ds.name: output_dr}\n",
    "    \n",
    "\n",
    "# Specify command line arguments\n",
    "arguments = ['--input_data_dir', str(input_ds.as_download()),\n",
    "             '--output_data_dir', output_dir,\n",
    "             '--file_name', 'aerial_bottleneck_resnet50_brainwave.h5']\n",
    "\n",
    "\n",
    "# Configure the script \n",
    "src = ScriptRunConfig(source_directory=script_folder, \n",
    "                      script=script_name, \n",
    "                      run_config=run_config, \n",
    "                      arguments=arguments \n",
    "                     ) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit the run and start RunDetails widget."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2b05e405e9d4dc9908513e3ddea3dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "_UserRunWidget(widget_settings={'childWidgetDisplay': 'popup', 'send_telemetry': True, 'log_level': 'INFO', 's…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from azureml.widgets import RunDetails\n",
    "\n",
    "tags = {\"Compute target\": \"AML Compute GPU\", \"DNN\": \"Brainwave ResNet50\"}\n",
    "run = exp.submit(src)\n",
    "\n",
    "RunDetails(run).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Block to wait till the run finishes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'runId': 'aerial-feature-engineering_1544056948838',\n",
       " 'target': 'gpucompute',\n",
       " 'status': 'Finalizing',\n",
       " 'startTimeUtc': '2018-12-06T00:42:48.125762Z',\n",
       " 'properties': {'azureml.runsource': 'experiment',\n",
       "  'ContentSnapshotId': 'f7c7a2f1-07ac-4572-ba3f-1afe825c3304'},\n",
       " 'runDefinition': {'Script': 'extract.py',\n",
       "  'Arguments': ['--input_data_dir',\n",
       "   '$AZUREML_DATAREFERENCE_input_images',\n",
       "   '--output_data_dir',\n",
       "   'bottleneck_features',\n",
       "   '--file_name',\n",
       "   'aerial_bottleneck_resnet50_brainwave.h5'],\n",
       "  'SourceDirectoryDataStore': None,\n",
       "  'Framework': 0,\n",
       "  'Communicator': 0,\n",
       "  'Target': 'gpucompute',\n",
       "  'DataReferences': {'input_images': {'DataStoreName': 'input_images',\n",
       "    'Mode': 'Download',\n",
       "    'PathOnDataStore': None,\n",
       "    'PathOnCompute': 'aerial',\n",
       "    'Overwrite': True},\n",
       "   'workspaceblobstore': {'DataStoreName': 'workspaceblobstore',\n",
       "    'Mode': 'Upload',\n",
       "    'PathOnDataStore': 'bottleneck_features',\n",
       "    'PathOnCompute': 'bottleneck_features',\n",
       "    'Overwrite': True}},\n",
       "  'JobName': None,\n",
       "  'AutoPrepareEnvironment': True,\n",
       "  'MaxRunDurationSeconds': None,\n",
       "  'NodeCount': 1,\n",
       "  'Environment': {'Python': {'InterpreterPath': 'python',\n",
       "    'UserManagedDependencies': False,\n",
       "    'CondaDependencies': {'name': 'project_environment',\n",
       "     'dependencies': ['python=3.6.2',\n",
       "      {'pip': ['azureml-defaults==1.0.2']},\n",
       "      'h5py',\n",
       "      'tqdm',\n",
       "      'tensorflow-gpu==1.10']},\n",
       "    'CondaDependenciesFile': None},\n",
       "   'EnvironmentVariables': {'EXAMPLE_ENV_VAR': 'EXAMPLE_VALUE'},\n",
       "   'Docker': {'BaseImage': 'mcr.microsoft.com/azureml/base-gpu:0.2.0',\n",
       "    'Enabled': True,\n",
       "    'SharedVolumes': True,\n",
       "    'Preparation': None,\n",
       "    'GpuSupport': False,\n",
       "    'Arguments': [],\n",
       "    'BaseImageRegistry': {'Address': None,\n",
       "     'Username': None,\n",
       "     'Password': None}},\n",
       "   'Spark': {'Repositories': ['https://mmlspark.azureedge.net/maven'],\n",
       "    'Packages': [{'Group': 'com.microsoft.ml.spark',\n",
       "      'Artifact': 'mmlspark_2.11',\n",
       "      'Version': '0.12'}],\n",
       "    'PrecachePackages': True}},\n",
       "  'History': {'OutputCollection': True},\n",
       "  'Spark': {'Configuration': {'spark.app.name': 'Azure ML Experiment',\n",
       "    'spark.yarn.maxAppAttempts': '1'}},\n",
       "  'BatchAi': {'NodeCount': 0},\n",
       "  'AmlCompute': {'Name': None,\n",
       "   'VmSize': None,\n",
       "   'VmPriority': None,\n",
       "   'RetainCluster': False,\n",
       "   'ClusterMaxNodeCount': 1},\n",
       "  'Tensorflow': {'WorkerCount': 1, 'ParameterServerCount': 1},\n",
       "  'Mpi': {'ProcessCountPerNode': 1},\n",
       "  'Hdi': {'YarnDeployMode': 2},\n",
       "  'ContainerInstance': {'Region': None, 'CpuCores': 0, 'MemoryGb': 0},\n",
       "  'ExposedPorts': None,\n",
       "  'PrepareEnvironment': None},\n",
       " 'logFiles': {'azureml-logs/60_control_log.txt': 'https://jkamlslastoragevfzvtchj.blob.core.windows.net/azureml/ExperimentRun/aerial-feature-engineering_1544056948838/azureml-logs/60_control_log.txt?sv=2018-03-28&sr=b&sig=RqgwuTDNRi21LEzn7xr%2FhEN9YQHg2s%2FWqLVUHufzDy0%3D&st=2018-12-06T00%3A39%3A45Z&se=2018-12-06T08%3A49%3A45Z&sp=r',\n",
       "  'azureml-logs/azureml.log': 'https://jkamlslastoragevfzvtchj.blob.core.windows.net/azureml/ExperimentRun/aerial-feature-engineering_1544056948838/azureml-logs/azureml.log?sv=2018-03-28&sr=b&sig=tkOBxOWBekx8VyhdnFx%2BbDPqctgq5eRK2WwwSz31nOU%3D&st=2018-12-06T00%3A39%3A45Z&se=2018-12-06T08%3A49%3A45Z&sp=r',\n",
       "  'azureml-logs/80_driver_log.txt': 'https://jkamlslastoragevfzvtchj.blob.core.windows.net/azureml/ExperimentRun/aerial-feature-engineering_1544056948838/azureml-logs/80_driver_log.txt?sv=2018-03-28&sr=b&sig=xUn6bLAwa3W8OJLboPHilNCKc2OszbWt4o%2BNNMznaOw%3D&st=2018-12-06T00%3A39%3A45Z&se=2018-12-06T08%3A49%3A45Z&sp=r'}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "run.wait_for_completion(show_output=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After the run, AML copied the output bottleneck files to the default datastore. You can verify it using Azure Portal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean up resources\n",
    "\n",
    "Before you move to the next step, you can delete the GPU VM. We will not need it anymore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compute_target.delete()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Next Step\n",
    "The run has completed. You are ready to move to the next part of the lab in which you are going to train a multinomial classification model using the bottleneck features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
